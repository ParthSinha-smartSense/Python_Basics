{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "JUmFRfNle_km"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " BackPropogation"
      ],
      "metadata": {
        "id": "e_aJSndoh1j9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x=torch.tensor(1.0)\n",
        "y=torch.tensor(2.0)\n",
        "w=torch.tensor(1.0, requires_grad=True)"
      ],
      "metadata": {
        "id": "DgoO5Yj5gf28"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#forward pass\n",
        "y_hat= w*x\n",
        "loss= (y-y_hat)**2\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BL4pBB-7iPZH",
        "outputId": "147e32ab-448b-4be6-fa5e-a4b698c8658b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1., grad_fn=<PowBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#backward pass\n",
        "loss.backward()\n",
        "print(w.grad)\n",
        "\n",
        "##update weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cp5ks5iCidY2",
        "outputId": "02e57524-9328-41ed-f68b-04e883e6fa8c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(-2.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GD"
      ],
      "metadata": {
        "id": "2N82GaBlj-Eo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x=np.array([1,2,3,4],dtype=np.float32)\n",
        "y=np.array([2,4,6,8],dtype=np.float32)\n",
        "w=0.0\n",
        "\n",
        "\n",
        "def forward(x):\n",
        "  return w*x\n",
        "\n",
        "def loss(y,y_pred):\n",
        "  return ((y-y_pred)**2).mean()\n",
        "\n",
        "def gradient(x,y,y_pred):\n",
        "  return np.mean(2*x*(y_pred-y))\n",
        "\n",
        "\n",
        "print(f'Before training {forward(5)}')\n",
        "\n",
        "\n",
        "learning_rate=0.01\n",
        "n_iters=50\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  y_pred=forward(x)\n",
        "  l=loss(y,y_pred)\n",
        "  dw=gradient(x,y,y_pred)\n",
        "  w-=learning_rate*dw\n",
        "\n",
        "print(f'After training {forward(5)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WYwf6K8kAx_",
        "outputId": "edfc9c39-b8e6-476f-9b32-4206414d514a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before training 0.0\n",
            "After training 9.997042402625084\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using torch"
      ],
      "metadata": {
        "id": "6u33MiMwmSdY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x=torch.tensor([1,2,3,4],dtype=torch.float32)\n",
        "y=torch.tensor([2,4,6,8],dtype=torch.float32)\n",
        "w=torch.tensor([0.0],dtype=torch.float32,requires_grad=True)\n",
        "\n",
        "\n",
        "def forward(x):\n",
        "  return w*x\n",
        "\n",
        "def loss(y,y_pred):\n",
        "  return ((y-y_pred)**2).mean()\n",
        "\n",
        "\n",
        "print(f'Before training {forward(5)}')\n",
        "\n",
        "\n",
        "learning_rate=0.01\n",
        "n_iters=50\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  y_pred=forward(x)\n",
        "  l=loss(y,y_pred)\n",
        "  l.backward()\n",
        "  with torch.no_grad():\n",
        "    w -= learning_rate * w.grad\n",
        "    # zero the gradients after updating\n",
        "    w.grad.zero_()\n",
        "\n",
        "print(f'After training {forward(5)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wTJxUaVmUO0",
        "outputId": "3cd6153e-5265-4639-a19d-36be5d5892c6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before training tensor([0.], grad_fn=<MulBackward0>)\n",
            "tensor([0., 0., 0., 0.], grad_fn=<MulBackward0>)\n",
            "tensor([0.3000, 0.6000, 0.9000, 1.2000], grad_fn=<MulBackward0>)\n",
            "tensor([0.5550, 1.1100, 1.6650, 2.2200], grad_fn=<MulBackward0>)\n",
            "tensor([0.7717, 1.5435, 2.3152, 3.0870], grad_fn=<MulBackward0>)\n",
            "tensor([0.9560, 1.9120, 2.8680, 3.8239], grad_fn=<MulBackward0>)\n",
            "tensor([1.1126, 2.2252, 3.3378, 4.4504], grad_fn=<MulBackward0>)\n",
            "tensor([1.2457, 2.4914, 3.7371, 4.9828], grad_fn=<MulBackward0>)\n",
            "tensor([1.3588, 2.7177, 4.0765, 5.4354], grad_fn=<MulBackward0>)\n",
            "tensor([1.4550, 2.9100, 4.3651, 5.8201], grad_fn=<MulBackward0>)\n",
            "tensor([1.5368, 3.0735, 4.6103, 6.1471], grad_fn=<MulBackward0>)\n",
            "tensor([1.6063, 3.2125, 4.8188, 6.4250], grad_fn=<MulBackward0>)\n",
            "tensor([1.6653, 3.3306, 4.9959, 6.6613], grad_fn=<MulBackward0>)\n",
            "tensor([1.7155, 3.4310, 5.1465, 6.8621], grad_fn=<MulBackward0>)\n",
            "tensor([1.7582, 3.5164, 5.2746, 7.0328], grad_fn=<MulBackward0>)\n",
            "tensor([1.7945, 3.5889, 5.3834, 7.1778], grad_fn=<MulBackward0>)\n",
            "tensor([1.8253, 3.6506, 5.4759, 7.3012], grad_fn=<MulBackward0>)\n",
            "tensor([1.8515, 3.7030, 5.5545, 7.4060], grad_fn=<MulBackward0>)\n",
            "tensor([1.8738, 3.7475, 5.6213, 7.4951], grad_fn=<MulBackward0>)\n",
            "tensor([1.8927, 3.7854, 5.6781, 7.5708], grad_fn=<MulBackward0>)\n",
            "tensor([1.9088, 3.8176, 5.7264, 7.6352], grad_fn=<MulBackward0>)\n",
            "tensor([1.9225, 3.8450, 5.7674, 7.6899], grad_fn=<MulBackward0>)\n",
            "tensor([1.9341, 3.8682, 5.8023, 7.7364], grad_fn=<MulBackward0>)\n",
            "tensor([1.9440, 3.8880, 5.8320, 7.7760], grad_fn=<MulBackward0>)\n",
            "tensor([1.9524, 3.9048, 5.8572, 7.8096], grad_fn=<MulBackward0>)\n",
            "tensor([1.9595, 3.9191, 5.8786, 7.8381], grad_fn=<MulBackward0>)\n",
            "tensor([1.9656, 3.9312, 5.8968, 7.8624], grad_fn=<MulBackward0>)\n",
            "tensor([1.9708, 3.9415, 5.9123, 7.8831], grad_fn=<MulBackward0>)\n",
            "tensor([1.9751, 3.9503, 5.9254, 7.9006], grad_fn=<MulBackward0>)\n",
            "tensor([1.9789, 3.9578, 5.9366, 7.9155], grad_fn=<MulBackward0>)\n",
            "tensor([1.9820, 3.9641, 5.9461, 7.9282], grad_fn=<MulBackward0>)\n",
            "tensor([1.9847, 3.9695, 5.9542, 7.9390], grad_fn=<MulBackward0>)\n",
            "tensor([1.9870, 3.9741, 5.9611, 7.9481], grad_fn=<MulBackward0>)\n",
            "tensor([1.9890, 3.9779, 5.9669, 7.9559], grad_fn=<MulBackward0>)\n",
            "tensor([1.9906, 3.9813, 5.9719, 7.9625], grad_fn=<MulBackward0>)\n",
            "tensor([1.9920, 3.9841, 5.9761, 7.9681], grad_fn=<MulBackward0>)\n",
            "tensor([1.9932, 3.9865, 5.9797, 7.9729], grad_fn=<MulBackward0>)\n",
            "tensor([1.9942, 3.9885, 5.9827, 7.9770], grad_fn=<MulBackward0>)\n",
            "tensor([1.9951, 3.9902, 5.9853, 7.9804], grad_fn=<MulBackward0>)\n",
            "tensor([1.9958, 3.9917, 5.9875, 7.9834], grad_fn=<MulBackward0>)\n",
            "tensor([1.9965, 3.9929, 5.9894, 7.9859], grad_fn=<MulBackward0>)\n",
            "tensor([1.9970, 3.9940, 5.9910, 7.9880], grad_fn=<MulBackward0>)\n",
            "tensor([1.9974, 3.9949, 5.9923, 7.9898], grad_fn=<MulBackward0>)\n",
            "tensor([1.9978, 3.9957, 5.9935, 7.9913], grad_fn=<MulBackward0>)\n",
            "tensor([1.9982, 3.9963, 5.9945, 7.9926], grad_fn=<MulBackward0>)\n",
            "tensor([1.9984, 3.9969, 5.9953, 7.9937], grad_fn=<MulBackward0>)\n",
            "tensor([1.9987, 3.9973, 5.9960, 7.9947], grad_fn=<MulBackward0>)\n",
            "tensor([1.9989, 3.9977, 5.9966, 7.9955], grad_fn=<MulBackward0>)\n",
            "tensor([1.9990, 3.9981, 5.9971, 7.9961], grad_fn=<MulBackward0>)\n",
            "tensor([1.9992, 3.9984, 5.9975, 7.9967], grad_fn=<MulBackward0>)\n",
            "tensor([1.9993, 3.9986, 5.9979, 7.9972], grad_fn=<MulBackward0>)\n",
            "After training tensor([9.9970], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "x = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
        "y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
        "\n",
        "\n",
        "n_samples, n_features = x.shape\n",
        "\n",
        "x_test = torch.tensor([5], dtype=torch.float32)\n",
        "\n",
        "input_size = n_features\n",
        "output_size = n_features\n",
        "\n",
        "\n",
        "class LinearRegression(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(LinearRegression, self).__init__()\n",
        "        # define diferent layers\n",
        "        self.lin = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lin(x)\n",
        "\n",
        "model = LinearRegression(input_size, output_size)\n",
        "\n",
        "print(f'Prediction before training: f(5) = {model(x_test).item():.3f}')\n",
        "\n",
        "\n",
        "learning_rate = 0.01\n",
        "n_iters = 100\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "for epoch in range(n_iters):\n",
        "    # predict = forward pass with our model\n",
        "    y_predicted = model(x)\n",
        "\n",
        "    # loss\n",
        "    l = loss(y, y_predicted)\n",
        "\n",
        "    # calculate gradients = backward pass\n",
        "    l.backward()\n",
        "\n",
        "    # update weights\n",
        "    optimizer.step()\n",
        "\n",
        "    # zero the gradients after updating\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        [w, b] = model.parameters() # unpack parameters\n",
        "        print('epoch ', epoch+1, ': w = ', w[0][0].item(), ' loss = ', l)\n",
        "\n",
        "print(f'Prediction after training: f(5) = {model(x_test).item():.3f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0m0S96AvWkO",
        "outputId": "b99cf453-91eb-4ccd-e60a-ef9b8ff5c96b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: f(5) = 3.163\n",
            "epoch  1 : w =  0.6409040093421936  loss =  tensor(11.8180, grad_fn=<MseLossBackward0>)\n",
            "epoch  11 : w =  1.4318721294403076  loss =  tensor(0.5537, grad_fn=<MseLossBackward0>)\n",
            "epoch  21 : w =  1.569422960281372  loss =  tensor(0.2478, grad_fn=<MseLossBackward0>)\n",
            "epoch  31 : w =  1.601567268371582  loss =  tensor(0.2263, grad_fn=<MseLossBackward0>)\n",
            "epoch  41 : w =  1.6164610385894775  loss =  tensor(0.2130, grad_fn=<MseLossBackward0>)\n",
            "epoch  51 : w =  1.6282930374145508  loss =  tensor(0.2006, grad_fn=<MseLossBackward0>)\n",
            "epoch  61 : w =  1.639353632926941  loss =  tensor(0.1889, grad_fn=<MseLossBackward0>)\n",
            "epoch  71 : w =  1.650019645690918  loss =  tensor(0.1779, grad_fn=<MseLossBackward0>)\n",
            "epoch  81 : w =  1.6603597402572632  loss =  tensor(0.1675, grad_fn=<MseLossBackward0>)\n",
            "epoch  91 : w =  1.6703927516937256  loss =  tensor(0.1578, grad_fn=<MseLossBackward0>)\n",
            "Prediction after training: f(5) = 9.339\n"
          ]
        }
      ]
    }
  ]
}